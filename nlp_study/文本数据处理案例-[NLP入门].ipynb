{"cells":[{"source":"# 简介\n实现任何程度或者级别的人工智能所必需的最大突破之一就是拥有可以处理文本数据的机器。值得庆幸的是，全世界文本数据的数量在最近几年已经实现指数级增长。这也迫切需要人们从文本数据中挖掘新知识、新观点。从社交媒体分析到风险管理和网络犯罪保护，处理文本数据已经变得前所未有的重要。\n\n在这篇文章中，我们将要讨论不同的特征提取方法，从一些基本技巧逐步深入学习高级自然语言处理技术。我们也将会学习如何预处理文本数据，以便可以从“干净”数据中提取更好的特征。\n\n# 目录\n1 文本数据的基本体征提取\n* 词汇数量\n* 字符数量\n* 平均字长\n* 停用词数量\n* 特殊字符数量\n* 数字数量\n* 大写字母数量\n\n2 文本数据的基本预处理\n* 小写转换\n* 去除标点符号\n* 去除停用词\n* 去除频现词\n* 去除稀疏词\n* 拼写校正\n* 分词(tokenization)\n* 词干提取(stemming)\n* 词形还原(lemmatization)\n\n3 高级文本处理\n* N-grams语言模型\n* 词频\n* 逆文档频率\n* TF-IDF\n* 词袋\n* 情感分析\n* 词嵌入\n\n","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"id":"CC3622B21E95424B8F3AB517230C897C","mdEditEnable":false}},{"source":"## 1 基本特征提取\n即使我们对NLP没有充足的知识储备，但是我们可以使用python来提取文本数据的几个基本特征。在开始之前，我们使用pandas将数据集加载进来，以便后面其他任务的使用，数据集是Twitter情感文本数据集。\n\n```python\nimport pandas as pd\ntrain=pd.read_csv(\"files/data/python46-data/train_E6oV3lV.csv\")\nprint(train.head(10))\n```\n\n","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"id":"087180EE13974D968577BBADCAA2BD19","mdEditEnable":false}},{"source":"```python\n  id  label                                              tweet\n0   1      0   @user when a father is dysfunctional and is s...\n1   2      0  @user @user thanks for #lyft credit i can't us...\n2   3      0                                bihday your majesty\n3   4      0  #model   i love u take with u all the time in ...\n4   5      0             factsguide: society now    #motivation\n5   6      0  [2/2] huge fan fare and big talking before the...\n6   7      0   @user camping tomorrow @user @user @user @use...\n7   8      0  the next school year is the year for exams.ð��...\n8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n9  10      0   @user @user welcome here !  i'm   it's so #gr...\n```","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"id":"6E6AA668DD604CFD811278D3C043781E","mdEditEnable":false}},{"source":"### 1.1 词汇数量\n对每一条推文，我们可以提取的最基本特征之一就是词语数量。这样做的初衷就是通常情况下，负面情绪评论含有词语数量比正面情绪评论多。\n\n我们可以简单地调用split函数，将句子切分：\n```python\ntrain['word_count']=train['tweet'].apply(lambda x:len(str(x).split(\" \")))\ntrain[['tweet','word_count']].head()\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmc2fmjb.png)\n\n","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"id":"8077C42E0F7149D6BC965D3E9E71B662","mdEditEnable":false}},{"metadata":{"id":"79A70DE76F954FDBAF19CE6002B50836","mdEditEnable":false},"cell_type":"markdown","source":"### 1.2 字符数量\n选择字符数量作为特征的原因和前一个特征一样。在这里，我们直接通过字符串长度计算每条推文字符数量\n```python\ntrain['char_count']=train['tweet'].str.len()\ntrain[['tweet','char_count']].head()\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmf8d86f.png)\n\n注意这里字符串的个数包含了推文中的空格个数，我们根据需要自行去除掉"},{"metadata":{"id":"377C42E6BC8C45FA93D210C8464B3E04","mdEditEnable":false},"cell_type":"markdown","source":"### 1.3 平均词汇长度\n我们接下来将计算每条推文的平均词汇长度作为另一个特征，这个有可能帮助我们改善模型。将每条推文所有单词的长度然后除以每条推文单词的个数，即可作为平均词汇长度。\n```python\ndef avg_word(sentence):\n    words=sentence.split()\n    return (sum(len(word) for word in words)/len(words))\n\ntrain['avg_word']=train['tweet'].apply(lambda x:avg_word(x))\ntrain[['tweet','avg_word']].head()\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmgxas68.png)\n"},{"metadata":{"id":"B634BE8C36DE4095864378FB84F61D37","mdEditEnable":false},"cell_type":"markdown","source":"### 1.4 停用词的数量\n通常情况下，在解决NLP问题时，首要任务时去除停用词（stopword）。但是有时计算停用词的数量可以提供我们之前失去的额外信息。下面关于停用词的解释：\n\n为节省存储空间和提高搜索效率，搜索引擎在索引页面或处理搜索请求时会自动忽略某些字或词，这些字或词即被称为Stop Words(停用词)。通常意义上，Stop Words大致为如下两类：\n* 这些词应用十分广泛，在Internet上随处可见，比如“Web”一词几乎在每个网站上均会出现，对这样的词搜索引擎无 法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率；\n* 这类就更多了，包括了语气助词、副词、介词、连接词等，通常自身 并无明确的意义，只有将其放入一个完整的句子中才有一定作用，如常见的“的”、“在”之类。\n\n在这里，我们导入NLTK库中的stopwors模块\n```python\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\ntrain['stopwords']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x in stop]))\ntrain[['tweet','stopwords']].head()\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmj39wqq.png)\n"},{"metadata":{"id":"52E3FEF551F443EDA232018967088C5B","mdEditEnable":false},"cell_type":"markdown","source":"### 1.5 特殊字符的数量\n一个比较有趣的特征就是我们可以从每个推文中提取“#”和“@”符号的数量。这也有利于我们从文本数据中提取更多信息\n\n这里我们使用`startswith`函数来处理\n```python\ntrain['hashtags']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x.startswith(\"#\")]))\ntrain[['tweet','hashtags']].head()\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmks8wn0.png)\n"},{"metadata":{"id":"D51B1307AE6340C88A78CF9AEBC6DE98","mdEditEnable":false},"cell_type":"markdown","source":"### 1.6 数字的数量\n这个特征并不常用，但是在做相似任务时，数字数量是一个比较有用的特征\n```python\ntrain['numerics']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x.isdigit()]))\ntrain[['tweet','numerics']].head()\n```\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmm3bw27.png)\n"},{"metadata":{"id":"72F2086703544FAC86BCF5F6E677C753","mdEditEnable":false},"cell_type":"markdown","source":"### 1.7 大写单词的数量\n“Anger”或者 “Rage”通常情况下使用大写来表述，所以有必要去识别出这些词\n```python\ntrain['upper']=train['tweet'].apply(lambda sen:len([x for x in sen.split() if x.isupper()]))\ntrain[['tweet','upper']].head()\n```\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsmnwqspf.png)\n"},{"metadata":{"id":"D06BDF5D12AE49D180A0AA8D12B43245","mdEditEnable":false},"cell_type":"markdown","source":"## 2 文本数据的预处理\n到目前为止,我们已经学会了如何从文本数据中提取基本特征。深入文本和特征提取之前,我们的第一步应该是清洗数据,以获得更好的特性。\n我们将实现这一目标做一些基本的训练数据预处理步骤。\n\n### 2.1 小写转化\n预处理的第一步,我们要做的是把我们的推文变成小写。这避免了拥有相同的多个副本。例如,当我们计算字词汇数量时,“Analytics”和“analytics”将被视为不同的单词。\n\n```python\ntrain['tweet']=train['tweet'].apply(lambda sen:\" \".join(x.lower() for x in sen.split()))\ntrain['tweet'].head()\n```"},{"metadata":{"id":"91349C67B3DC4B829ABD88D83F41CFFA","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    @user when a father is dysfunctional and is so...\n1    @user @user thanks for #lyft credit i can't us...\n2                                  bihday your majesty\n3    #model i love u take with u all the time in ur...\n4                  factsguide: society now #motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"484504F6719945B2AB4E6070B8F778C6","mdEditEnable":false},"cell_type":"markdown","source":"### 2.2 去除标点符号\n下一步是去除标点符号,因为它在文本数据中不添加任何额外的信息。因此删除的所有符号将帮助我们减少训练数据的大小。\n```python\ntrain['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\ntrain['tweet'].head()\n```"},{"metadata":{"id":"3842AB2080DC4FAC8C72FE22C56B1285","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    user when a father is dysfunctional and is so ...\n1    user user thanks for lyft credit i cant use ca...\n2                                  bihday your majesty\n3    model i love u take with u all the time in urð...\n4                    factsguide society now motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"0E4C10E2D1A74D7780B9DAA3F706C46F","mdEditEnable":false},"cell_type":"markdown","source":"正如你所看到的在上面的输出中,所有的标点符号,包括\"#\"和\"@\"已经从训练数据中去除\n\n### 2.3 停用词去除\n正如我们前面所讨论的,停止词(或常见单词)应该从文本数据中删除。为了这个目的,我们可以创建一个列表stopwords作为自己停用词库或我们可以使用预定义的库。\n```python\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\ntrain['tweet']=train['tweet'].apply(lambda sen:\" \".join(x for x in sen.split() if x not in stop))\ntrain['tweet'].head()\n```"},{"metadata":{"id":"0D68DDCDA5EF40E0BAABAFE58ED14A3F","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    user father dysfunctional selfish drags kids d...\n1    user user thanks lyft credit cant use cause do...\n2                                       bihday majesty\n3                model love u take u time urð ðððð ððð\n4                        factsguide society motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"77ED1D8D224C4A919D3A4D8A86DF4CFB","mdEditEnable":false},"cell_type":"markdown","source":"### 2.4 常见词去除\n我们可以把常见的单词从文本数据首先,让我们来检查中最常出现的10个字文本数据然后再调用删除或保留。\n```python\nfreq=pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\nfreq\n```"},{"metadata":{"id":"E721D6B68ED745C28B2952B2395F1F56","mdEditEnable":false},"cell_type":"markdown","source":"```\nuser     17473\nlove      2647\nð         2511\nday       2199\nâ         1797\nhappy     1663\namp       1582\nim        1139\nu         1136\ntime      1110\ndtype: int64\n```"},{"metadata":{"id":"8CB9EF48F0084FF2B6BE273946CBBC20","mdEditEnable":false},"cell_type":"markdown","source":"现在我们把这些词去除掉，因为它们对我们文本数据分类没有任何作用\n```python\nfreq=list(freq.index)\nfreq\n```"},{"metadata":{"id":"2D9D511387EE41FD8043E348E99FF4EB","mdEditEnable":false},"cell_type":"markdown","source":"```\n['user', 'love', 'ð', 'day', 'â', 'happy', 'amp', 'im', 'u', 'time']\n```"},{"metadata":{"id":"85D096DEF998460F945E8892897F1E1A","mdEditEnable":false},"cell_type":"markdown","source":"```python\ntrain['tweet']=train['tweet'].apply(lambda sen:' '.join(x for x in sen.split() if x not in freq))\ntrain['tweet'].head()\n```"},{"metadata":{"id":"F973C83EBF6B40DA8178DFB33BDCC52E","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    father dysfunctional selfish drags kids dysfun...\n1    thanks lyft credit cant use cause dont offer w...\n2                                       bihday majesty\n3                              model take urð ðððð ððð\n4                        factsguide society motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"B2EE3322BAE742598FF8CCE7C68CCAA7","mdEditEnable":true},"cell_type":"markdown","source":"### 2.5 稀缺词去除\n同样,正如我们删除最常见的话说,这一次让我们从文本中删除很少出现的词。因为它们很稀有,它们之间的联系和其他词主要是噪音。可以替换罕见的单词更一般的形式,然后这将有更高的计数。\n```python\nfreq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\nfreq\n```"},{"metadata":{"id":"EC881831981141238A7D51B9B52A2E3E","mdEditEnable":false},"cell_type":"markdown","source":"```\nhappenedâ       1\nbritmumspics    1\nlaterr          1\n2230            1\ndkweddking      1\nampsize         1\nmoviescenes     1\nkaderimsin      1\nnfinity         1\nbabynash        1\ndtype: int64\n```"},{"metadata":{"id":"11FF8267EC844A8E8B2C297B1E7B6FBB","mdEditEnable":false},"cell_type":"markdown","source":"```python\nfreq = list(freq.index)\ntrain['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['tweet'].head()\n```"},{"metadata":{"id":"270BFB0124B344CAB7C470A5DDBC000B","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    father dysfunctional selfish drags kids dysfun...\n1    thanks lyft credit cant use cause dont offer w...\n2                                       bihday majesty\n3                              model take urð ðððð ððð\n4                        factsguide society motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"80A42806323E475A80F5C41DA4C5EB08","mdEditEnable":false},"cell_type":"markdown","source":"所有这些预处理步骤是必不可少的,帮助我们减少我们的词汇噪音,这样最终产生更有效的特征。\n\n### 2.6 拼写校对\n我们都见过推文存在大量的拼写错误。我们再短时间内匆忙发送tweet,很难发现这些错误。在这方面,拼写校正是一个有用的预处理步骤,因为这也会帮助我们减少单词的多个副本。例如,“Analytics”和“analytcs”将被视为不同的单词,即使它们在同一意义上使用。\n\n为实现这一目标,我们将使用textblob库。\n\nTextBlob是一个用Python编写的开源的文本处理库。它可以用来执行很多自然语言处理的任务，比如，词性标注，名词性成分提取，情感分析，文本翻译，等等。你可以在官方文档阅读TextBlog的所有特性。\n\n```python\nfrom textblob import TextBlob\ntrain['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n```"},{"metadata":{"id":"3E65E605485E433B8810A575F68C13AE","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    father dysfunctional selfish drags kiss dysfun...\n1    thanks left credit can use cause dont offer wh...\n2                                       midday majesty\n3                               model take or ðððð ððð\n4                        factsguide society motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"3CA55D44E06440A688E86F9BB0D48864","mdEditEnable":false},"cell_type":"markdown","source":"注意,它会花费很多时间去做这些修正。因此,为了学习的目的,我只显示这种技术运用在前5行的效果。\n另外在使用这个技术之前，需要小心一些，因为如果推文中存在大量缩写，比如“your”缩写为“ur”，那么将修正为“or”\n\n### 2.7 分词\n分词是指将文本划分为一系列的单词或词语。在我们的示例中,我们使用了textblob库\n\n```python\nTextBlob(train['tweet'][1]).words\n```\n\n\n```python\nWordList(['thanks', 'lyft', 'credit', 'cant', 'use', 'cause', 'dont', 'offer', 'wheelchair', 'vans', 'pdx', 'disapointed', 'getthanked'])\n```\n"},{"metadata":{"id":"EEEAC40ECC0844A39330630A4E0F9F6D","mdEditEnable":false},"cell_type":"markdown","source":"### 2.8 词干提取\n词形还原（lemmatization），是把一个任何形式的语言词汇还原为一般形式（能表达完整语义），而词干提取（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义）。词形还原和词干提取是词形规范化的两类重要方式，都能够达到有效归并词形的目的，二者既有联系也有区别。具体介绍请参考词干提取（stemming）和词形还原（lemmatization）\n\n词干提取(stemming)是指通过基于规则的方法去除单词的后缀，比如“ing”,“ly”，“s”等等。\n\n```python\nfrom nltk.stem import PorterStemmer\nst=PorterStemmer()\ntrain['tweet'][:5].apply(lambda x:\" \".join([st.stem(word) for word in x.split()]))\n```"},{"metadata":{"id":"EEF48CEC43F84F7484FC2E438E1E78C6","mdEditEnable":false},"cell_type":"markdown","source":"```\n0        father dysfunct selfish drag kid dysfunct run\n1    thank lyft credit cant use caus dont offer whe...\n2                                       bihday majesti\n3                              model take urð ðððð ððð\n4                              factsguid societi motiv\nName: tweet, dtype: object\n```"},{"metadata":{"id":"2AB8ECB529A741948ABD7B7F069897CF","mdEditEnable":false},"cell_type":"markdown","source":"在上面的输出中，“dysfunctional ”已经变为“dysfunct ”\n\n### 2.9 词性还原\n词形还原处理后获得的结果是具有一定意义的、完整的词，一般为词典中的有效词\n```python\nfrom textblob import Word\ntrain['tweet']=train['tweet'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\ntrain['tweet'].head()\n```"},{"metadata":{"id":"050DDB4F08EF4FDE99D8CD9A4AA7D6D1","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    father dysfunctional selfish drag kid dysfunct...\n1    thanks lyft credit cant use cause dont offer w...\n2                                       bihday majesty\n3                              model take urð ðððð ððð\n4                        factsguide society motivation\nName: tweet, dtype: object\n```"},{"metadata":{"id":"9AF320FCF85240A79701012B1F42B16A","mdEditEnable":false},"cell_type":"markdown","source":"## 3 高级文本处理\n到目前为止,我们已经做了所有的可以清洗我们数据的预处理基本步骤。现在,我们可以继续使用NLP技术提取特征。\n\n### 3.1 N-grams\nN-grams称为N元语言模型，是多个词语的组合，是一种统计语言模型，用来根据前(n-1)个item来预测第n个item。常见模型有一元语言模型(unigrams)、二元语言模型（bigrams ）、三元语言模型(trigrams )。\n\nUnigrams包含的信息通常情况下比bigrams和trigrams少，需要根据具体应用选择语言模型，因为如果n-grams太短，这时不能捕获重要信息。另一方面，如果n-grams太长，那么捕获的信息基本上是一样的，没有差异性。\n\n```python\nTextBlob(train['tweet'][0]).ngrams(2)\n```"},{"metadata":{"id":"CDA217C611F149CE85154C50DB61ACF2","mdEditEnable":false},"cell_type":"markdown","source":"```\n[WordList(['father', 'dysfunctional']),\n WordList(['dysfunctional', 'selfish']),\n WordList(['selfish', 'drag']),\n WordList(['drag', 'kid']),\n WordList(['kid', 'dysfunction']),\n WordList(['dysfunction', 'run'])]\n```"},{"metadata":{"id":"63F3518F3DC346368AC3251AA2244172","mdEditEnable":false},"cell_type":"markdown","source":"### 3.2 词频\n词频(Term frequency)就是一个单词在一个句子出现的次数与这个句子单词个数的比例。\n** TF = (Number of times term T appears in the particular row) / (number of terms in that row)**\n```python\ntf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsn59q63h.png)\n"},{"metadata":{"id":"D02E993B245C4BB985B4BD60C8A79C0F","mdEditEnable":false},"cell_type":"markdown","source":"### 3.3 反转文档频率\n反转文档频率(Inverse Document Frequency)，简称为IDF，其原理可以简单理解为如果一个单词在所有文档都会出现，那么可能这个单词对我们没有那么重要。\n\n一个单词的IDF就是所有行数与出现该单词的行的个数的比例，最后对数。\nIDF = log(N/n)\n\n```python\nimport numpy as np\nfor i,word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] =np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))\ntf1\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsn6qd81c.png)\n"},{"metadata":{"id":"F4B1EDFC889043B7822B97E23469281B","mdEditEnable":false},"cell_type":"markdown","source":"### 3.4 词频-反转文档频率(TF-IDF)\nTF-IDF=TF`*`IDF\n\n```python\ntf1['tfidf']=tf1['tf']*tf1['idf']\ntf1\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsn88h905.png)\n\n我们可以看到，TF-IDF已经“惩罚了”‘don’t’, ‘can’t’, 和‘use’,因为它们是通用词，tf-idf的值都比较低。\n\n另外可以通过sksklearn直接计算tf-idf值\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['tweet'])\n\ntrain_vect\n```\n\n"},{"metadata":{"id":"5C4033A024AD40C99108FDB637AAFC7B","mdEditEnable":false},"cell_type":"markdown","source":"```\n<31962x1000 sparse matrix of type '<class 'numpy.float64'>'\n    with 114055 stored elements in Compressed Sparse Row format>\n```\n"},{"metadata":{"id":"23CC1E52DE9F484A8DD97496BAB456C3","mdEditEnable":false},"cell_type":"markdown","source":"### 3.5 词袋\nBOW，就是将文本/Query看作是一系列词的集合。由于词很多，所以咱们就用袋子把它们装起来，简称词袋。至于为什么用袋子而不用筐（basket）或者桶（bucket），这咱就不知道了。举个例子：\n\n> 文本1：苏宁易购/是/国内/著名/的/B2C/电商/之一\n\n这是一个短文本。“/”作为词与词之间的分割。从中我们可以看到这个文本包含“苏宁易购”，“B2C”，“电商”等词。换句话说，该文本的的词袋由“苏宁易购”，“电商”等词构成。\n\n详细请参考词袋模型和词向量模型\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['tweet'])\ntrain_bow\n```"},{"metadata":{"id":"F2BA6CF413BC42E1A53CCFE6A227BE58","mdEditEnable":false},"cell_type":"markdown","source":"```\n<31962x1000 sparse matrix of type '<class 'numpy.int64'>'\n    with 128402 stored elements in Compressed Sparse Row format>\n```"},{"metadata":{"id":"05F9601646574592B0E95CE4F1E7F455","mdEditEnable":false},"cell_type":"markdown","source":"### 3.6 情感分析\n我们最终需要解决的任务就是如何对推文进行情感分析，在使用ML/DL模型之前，我们可以使用textblob库去进行评测情感\n```python\ntrain['tweet'][:5].apply(lambda x:TextBlob(x).sentiment)\n```"},{"metadata":{"id":"9AE6669681B94F7A87B6A2803601B999","mdEditEnable":false},"cell_type":"markdown","source":"```\n0    (-0.3, 0.5354166666666667)\n1                    (0.2, 0.2)\n2                    (0.0, 0.0)\n3                    (0.0, 0.0)\n4                    (0.0, 0.0)\nName: tweet, dtype: object\n```"},{"metadata":{"id":"E9457BC78D3745A795E568CB69839611","mdEditEnable":false},"cell_type":"markdown","source":"使用TextBlob情感分析的结果，以元组的方式进行返回，形式如(polarity, subjectivity). 其中polarity的分数是一个范围为 [-1.0 , 1.0 ] 浮点数, 正数表示积极，负数表示消极。subjectivity 是一个 范围为 [0.0 , 1.0 ] 的浮点数，其中 0.0 表示 客观，1.0表示主观的。\n\n下面是一个简单实例\n\n```python\nfrom textblob import TextBlob\ntestimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\nprint(testimonial.sentiment) \n```"},{"metadata":{"id":"C0372859D83B4A5FA6642CBF7204BF7B","mdEditEnable":false},"cell_type":"markdown","source":"```\nSentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)\n```"},{"metadata":{"id":"7884B8C8B9464B89A3A90E50576104BE","mdEditEnable":false},"cell_type":"markdown","source":"```python\ntrain['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\ntrain[['id','tweet','sentiment']].head()\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/ptsnf0asg7.png)\n"},{"metadata":{"id":"B812B56BE96E458E9BBEF6A0A0D6755F","mdEditEnable":false},"cell_type":"markdown","source":"### 3.7 词嵌入\n词嵌入就是文本的向量化表示，潜在思想就是相似单词的向量之间的距离比较短。\n```python\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = 'glove.6B.100d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove2word2vec(glove_input_file, word2vec_output_file)\n```\n\n# 总结\n通过这篇文章，希望大家对文本数据处理步骤以及特征选择有了大致了解，推荐大家在这些基础之上，使用机器学习或者深度学习方法进行情感预测\n\n\n# 引用\n文章来自于https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n\n          \n        "}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}