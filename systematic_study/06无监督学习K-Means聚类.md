# 机器学习(六)：通俗易懂无监督学习K-Means聚类算法

> K-Means是一种无监督学习方法，用于将无标签的数据集进行聚类。其中K指集群的数量，Means表示寻找集群中心点的手段。

## 一、 无监督学习 K-Means

贴标签是需要花钱的。
所以人们研究处理无标签数据集的方法。
面对无标签的数据集，我们期望从数据中找出一定的规律。一种最简单也最快速的聚类算法应运而生---K-Means。
它的核心思想很简单：物以类聚。

**用直白的话简单解释它的算法执行过程如下：**

1. 随便选择K个中心点。
2. 把距离它足够近的数据吸纳为成员，聚成K个集群。
3. 各集群内部重新选择中心点，选择标准是按照距离取**均值**作为中心点。
4. 重复2、3步骤直到收敛。

### 1. K的选择

#### 1.1 惯性指标（inertia）

​	K-Means的惯性计算方式是，**每个样本与最接近的集群中心点的均方距离的总和**。

![image-20210327212907332](https://i.loli.net/2021/03/27/qtZkYlypiHUSPg3.png)

​	一般地，惯性越小模型越好，但伴随K值的增大，惯性下降的速度变的很慢，因此我们选择“肘部”的K值，作为最优的K值选择。

#### 1.2 轮廓系数指标（silhouette）

​	K-Means的轮廓系数计算方式是，**与集群内其他样本的平均距离记为a，与外部集群样本的平均距离记为b，轮廓系数(b-a)/max(a,b)**。

![image-20210327212937648](https://i.loli.net/2021/03/27/UxZ8CMlKVpXcaWn.png)

​	一般地，轮廓系数指标越大越好，我们可以看到当K为2、3时均可取得不错的聚类效果。

### 2. 自动选择K值

#### 2.1 简单理解贝叶斯定理

​	白话解释贝叶斯：**当有新的证据出现时，不确定的事情更加确信了。** 这里的“确信”是指对不确定的事情的信心程度。

公式(可忽略)：

![image-20210327213051415](https://i.loli.net/2021/03/27/9bKUs6aHzjiZq8X.png)

​	其中，P(A) 表示事件A发生的概率，P(A|B)表示事件B发生时事件A发生的概率。上面的公式中B就是所谓的证据。这里要注意的是，P(B)的出现让P(A|B)变的更确定了，并不是说概率变高了或者变低了。概率的高或者低都是一种确定。它是一种信心程度的体现。

#### 2.2 贝叶斯高斯混合模型

​	使用`BayesianGaussianMixture`方法，而无需指定明确的K值。

```python
from sklearn.mixture import BayesianGaussianMixture
bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)
y_pred = bgm.fit_predict(X)
np.round(bgm.weights_, 2)
```

​	输出： `array([0.4 , 0.33, 0.27, 0. , 0. , 0. , 0. , 0. , 0. , 0. ])`

​	以上代码的执行逻辑是，初始化10个集群，不断调整有关集群数贝叶斯先验知识，来将不必要的集群权重设为0（或接近0），来确定最终K值。

